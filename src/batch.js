import 'dotenv/config';
import fs from 'fs';
import path from 'path';
import keccak256 from 'keccak256';
import { MerkleTree } from 'merkletreejs';
import { fileURLToPath } from 'url';

import { readNdjson } from './tools/ndjson.js';
import {
  openDB, getAllAttestedRecordIds,
  upsertBatchHeader, insertBatchRecords,
  markBatchSent, markBatchConfirmed, markBatchFailed,
  getBatchHeader, listPendingBatchHeaders
} from './tools/db.js';

import { attestMerkleBatch, getReceipt } from './eas.js';

const __filename = fileURLToPath(import.meta.url);
const __dirname  = path.dirname(__filename);

const DIR_INPUT_RECORDS = process.env.DIR_INPUT_RECORDS;
const DIR_MERKLE        = process.env.DIR_MERKLE;
const APP_STATE         = process.env.APP_STATE;

const toHex = (buf) => '0x' + Buffer.from(buf).toString('hex');
const leafFromRecord = (rec) => keccak256(Buffer.from(JSON.stringify(rec)));
const batchIdNow = () => new Date().toISOString().replace(/[:]/g, '-').replace(/\.\d{3}Z$/, 'Z');

async function readAllRecords(dir) {
  if (!fs.existsSync(dir)) return [];
  const files = fs.readdirSync(dir).filter(f => f.endsWith('.ndjson'));
  const out = [];
  for (const f of files) {
    const arr = await readNdjson(path.resolve(dir, f));
    out.push(...arr);
  }
  // å»é‡
  const seen = new Set(); const uniq = [];
  for (const r of out) {
    if (!r?.RECORD_ID) continue;
    if (seen.has(r.RECORD_ID)) continue;
    seen.add(r.RECORD_ID);
    uniq.push(r);
  }
  return uniq;
}

// â€”â€” å®•æœºæ¢å¤ï¼šæ‰«æ pending æ‰¹æ¬¡å¤´ï¼Œä¾æ® tx_hash æŸ¥è¯¢ç¡®è®¤ â€”â€” //
async function recoverPendingBatches(db) {
  const pendings = await listPendingBatchHeaders(db);
  if (pendings.length === 0) return;

  console.log(`ğŸ©¹ Recovering ${pendings.length} pending batch(es)...`);
  for (const p of pendings) {
    const { batch_id, tx_hash } = p;
    if (!tx_hash) {
      console.log(`â„¹ï¸ Pending batch ${batch_id} has no tx_hash yet, will be handled by next run when re-sent.`);
      continue;
    }
    try {
      const receipt = await getReceipt(tx_hash);
      if (receipt && receipt.status === 1) {
        // EAS SDKçš„ uid æ— æ³•ä» receipt ç›´æ¥æ‹¿ï¼Œè¿™é‡Œå»ºè®®ä½ åœ¨é¦–æ¬¡å‘é€æ—¶å°±ä¿å­˜ uidã€‚
        // è‹¥å®•æœºå¯¼è‡´ uid ä¸¢å¤±ï¼Œå¯åç»­è¡¥åšâ€œäº‹ä»¶å›æº¯â€æ¥æ‰¾ uidï¼ˆéœ€è¦ EAS ABIï¼‰ã€‚
        // è¿™é‡Œæš‚æ—¶æŠŠ uid æ ‡æ³¨ä¸º 'unknown:recovered'ï¼Œæˆ–ç•™ç©ºå¾…åç»­è¡¥å…¨ã€‚
        await markBatchConfirmed(db, { batch_id, attestation_uid: 'unknown:recovered' });
        console.log(`âœ… Recovered confirmed: batch ${batch_id} (tx=${tx_hash})`);
      } else {
        console.log(`âŒ› Batch ${batch_id} not confirmed yet (tx=${tx_hash}).`);
      }
    } catch (e) {
      console.log(`âš ï¸ Recover check failed for batch ${batch_id}:`, e.message);
    }
  }
}

async function main() {
  const db = await openDB(APP_STATE);

  // 0) å¯åŠ¨å…ˆåšä¸€æ¬¡æ¢å¤
  await recoverPendingBatches(db);

  // 1) è¯»å–å…¨éƒ¨æ–‡ä»¶ï¼Œè¿‡æ»¤å·² attested çš„ record
  const all = await readAllRecords(DIR_INPUT_RECORDS);
  const attested = new Set(await getAllAttestedRecordIds(db));
  const newRecords = all.filter(r => !attested.has(r.RECORD_ID));

  if (newRecords.length === 0) {
    console.log('âœ… No new records to batch.');
    return;
  }

  // 2) ç”Ÿæˆæ‰¹æ¬¡
  const bid    = batchIdNow();
  const leaves = newRecords.map(leafFromRecord);
  const tree   = new MerkleTree(leaves, keccak256, { sortPairs: true });
  const root   = tree.getHexRoot();

  if (!fs.existsSync(DIR_MERKLE)) fs.mkdirSync(DIR_MERKLE, { recursive: true });
  const rootPath   = path.join(DIR_MERKLE, `root-${bid}.json`);
  const proofsPath = path.join(DIR_MERKLE, `proofs-${bid}.ndjson`);

  fs.writeFileSync(rootPath, JSON.stringify({
    batch_id: bid, root, count: newRecords.length, createdAt: new Date().toISOString()
  }, null, 2));

  const proofsLines = newRecords.map((rec, i) => {
    const leaf = leaves[i];
    const proof = tree.getHexProof(leaf);
    return JSON.stringify({ record_id: rec.RECORD_ID, leaf: toHex(leaf), proof });
  }).join('\n') + '\n';
  fs.writeFileSync(proofsPath, proofsLines, 'utf-8');

  console.log(`ğŸ§º New batch ${bid} | root=${root} | count=${newRecords.length}`);
  console.log(`ğŸ“„ root:   ${rootPath}`);
  console.log(`ğŸ“„ proofs: ${proofsPath}`);

  // 3) å…¥åº“ï¼šå…ˆå†™æ‰¹æ¬¡å¤´ + è®¢å•ï¼ˆpendingï¼‰
  await upsertBatchHeader(db, { batch_id: bid, merkle_root: root, proofs_cid: proofsPath });
  await insertBatchRecords(db, {
    batch_id: bid, merkle_root: root, proofs_cid: proofsPath,
    record_ids: newRecords.map(r => r.RECORD_ID)
  });

  // 4) å‘é€å‰å¯åšâ€œé“¾ä¸Šæ¢æµ‹â€é¿å…é‡å¤ï¼ˆå¦‚éœ€äº‹ä»¶å›æº¯å¯è®©æˆ‘åŠ ï¼‰
  //    å½“å‰å…ˆç›´æ¥å‘é€ï¼Œæ‹¿åˆ° txHash/uid ä»¥ä¾¿æ¢å¤
  try {
    const { uid, txHash } = await attestMerkleBatch({
      merkle_root: root,
      batch_id: bid,
      count: newRecords.length,
      proofs_pointer: proofsPath   // å…ˆç”¨æœ¬åœ°è·¯å¾„ï¼Œåç»­å¯åˆ‡æ¢ IPFS
    });

    // å‘é€åç«‹åˆ»å†™ tx_hashï¼ˆä¸ºæ¢å¤å‡†å¤‡ï¼‰
    await markBatchSent(db, { batch_id: bid, tx_hash: txHash });

    // ç­‰å¾…ç¡®è®¤åå›å¡« uid å¹¶æ ‡è®° confirmedï¼ˆattestMerkleBatch å·²ç» wait è¿‡ä¸€æ¬¡ï¼‰
    await markBatchConfirmed(db, { batch_id: bid, attestation_uid: uid });

    console.log(`âœ… Batch ${bid} confirmed. UID=${uid}, tx=${txHash}`);
  } catch (e) {
    await markBatchFailed(db, { batch_id: bid, error: e.message || String(e) });
    console.error(`âŒ Batch ${bid} failed to attest:`, e);
  }
}

main().catch(e => {
  console.error('âŒ run-batch failed:', e);
  process.exit(1);
});

